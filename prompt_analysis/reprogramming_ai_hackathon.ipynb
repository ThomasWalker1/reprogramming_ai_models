{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from goodfire import Client\n",
    "import goodfire\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(variant,prompt):\n",
    "    completion=\"\"\n",
    "    for token in client.chat.completions.create([{\"role\": \"user\", \"content\": prompt}],model=variant,stream=True,max_completion_tokens=150,top_p=0.05):\n",
    "        completion+=token.choices[0].delta.content\n",
    "    return completion\n",
    "\n",
    "def feature_to_json(top_features):\n",
    "    feature_json = {}\n",
    "    for feature in top_features:\n",
    "        feature_json.update({str(feature.feature): feature.activation})\n",
    "    \n",
    "    return feature_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOODFIRE_API_KEY = 'sk-goodfire-6pFV2uNVzPcn2pmEGUNfhXYVgPo81T89xJrkFJfElNlxFVPmiLGbLA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"few_shot_questions.json\",\"r\") as file:\n",
    "    questions=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = goodfire.Client(\n",
    "    GOODFIRE_API_KEY\n",
    "  )\n",
    "\n",
    "# Instantiate a model variant\n",
    "variant = goodfire.Variant(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoodfire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChatMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'meta-llama/Meta-Llama-3-8B-Instruct'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'meta-llama/Meta-Llama-3.1-70B-Instruct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoodfire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariantInterface\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_completion_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'<|eot_id|>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<|begin_of_text|>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m_ChatAPICompletions__system_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'You are a helpful assistant who should follow the users requests. Be brief and to the point, but also be friendly and engaging.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoodfire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChatCompletion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoodfire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingChatCompletionChunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Create a chat completion.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.9.6/envs/apart_hackathon/lib/python3.9/site-packages/goodfire/api/chat/client.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a clever question!\n",
      "\n",
      "Let's break it down step by step. Since the ratio of their ages is 7:9, that means Rahul's age is 9x and Sachin's age is 7x.\n",
      "\n",
      "We know that Sachin is 9 years younger than Rahul, so we can set up an equation:\n",
      "\n",
      "9x - 7x = 9\n",
      "\n",
      "Combine like terms:\n",
      "\n",
      "2x = 9\n",
      "\n",
      "Divide both sides by 2:\n",
      "\n",
      "x = 4.5\n",
      "\n",
      "So, Rahul's age is 9x = 9(4.5) = 40.5 years old.\n",
      "\n",
      "And Sachin's age is 7x = 7(4.5) = 31.5 years old.\n",
      "\n",
      "So, Sachin is 31.5 years old!"
     ]
    }
   ],
   "source": [
    "for token in client.chat.completions.create(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Sachin is younger than Rahul by 9 years. If the ratio of their ages is 7 : 9, what is the age of Sachin?\"}\n",
    "    ],\n",
    "    model=variant,\n",
    "    stream=True,\n",
    "    max_completion_tokens=1000\n",
    "):\n",
    "    print(token.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(question, topk=10):\n",
    "    \n",
    "    with_cot_context = client.features.inspect(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question[\"cot\"]+\"\\n\"+question[\"question\"]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": question[\"cot_answer\"]\n",
    "            },\n",
    "        ],\n",
    "        model=variant,\n",
    "    )\n",
    "    \n",
    "    without_cot_context = client.features.inspect(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question[\"direct\"]+\"\\n\"+question[\"question\"]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": question[\"direct_answer\"]\n",
    "            },\n",
    "        ],\n",
    "        model=variant,\n",
    "    )\n",
    "    \n",
    "    with_cot_top_features = with_cot_context.top(k=topk)\n",
    "    without_cot_top_features = without_cot_context.top(k=topk)\n",
    "    \n",
    "    question['with_cot_top_features'] = feature_to_json(with_cot_top_features)\n",
    "    question['without_cot_top_features'] = feature_to_json(without_cot_top_features)\n",
    "    \n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in top_features_list:\n",
    "    question[\"with_cot_top_features\"] = json.loads(question['with_cot_top_features'])\n",
    "    question[\"without_cot_top_features\"] = json.loads(question['without_cot_top_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [11:46<00:00, 35.33s/it]\n"
     ]
    }
   ],
   "source": [
    "pbar=tqdm(enumerate(questions),total=len(questions))\n",
    "top_features_list = []\n",
    "for (k,question) in pbar:\n",
    "    \n",
    "    updated_question = get_top_features(question)\n",
    "    top_features_list.append(updated_question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"top_features_questions.json\",\"w\",encoding=\"utf-8\") as file:\n",
    "     json.dump(top_features_list,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_cot_context = client.features.inspect(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\\nA: The answer (arabic numerals) is\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"8\"\n",
    "        },\n",
    "    ],\n",
    "    model=variant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_cot_context = client.features.inspect(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\\nA: Let's think step by step\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Sounds like a fun problem! Let's break it down step by step.\\nSince the juggler can juggle 16 balls, and half of them are golf balls, that means there are 16/2 = 8 golf balls.\\nAnd, half of the golf balls are blue, so we need to find half of 8, which is 8/2 = 4.\\nSo, there are 4 blue golf balls!\"\n",
    "        },\n",
    "    ],\n",
    "    model=variant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_cot_context.top(k=10)\n",
    "feature_to_json(without_cot_context.top(k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Feature(\\\\\"The model initiating step-by-step explanation\\\\\")\": 1.433799342105263, \"Feature(\\\\\"Arithmetic operations in word problems\\\\\")\": 1.1082763671875, \"Feature(\\\\\"Spherical play objects, especially bouncing balls\\\\\")\": 1.0290305397727273, \"Feature(\\\\\"Types of car transmissions, especially automatic and semi-automatic\\\\\")\": 0.8590666118421053, \"Feature(\\\\\"Small countable objects\\\\\")\": 0.83154296875, \"Feature(\\\\\"Numerical values in math problems or quantitative descriptions\\\\\")\": 0.8077713815789473, \"Feature(\\\\\"Basic Arithmetic and Simple Calculations\\\\\")\": 0.7566856971153846, \"Feature(\\\\\"Math word problems involving financial calculations\\\\\")\": 0.5727796052631579, \"Feature(\\\\\"Mathematical word problems involving counting animals or body parts\\\\\")\": 0.4872036637931034, \"Feature(\\\\\"The model is formulating mathematical equations from word problems or scenarios\\\\\")\": 0.4193522135416667}'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(feature_to_json(with_cot_context.top(k=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureActivations(\n",
       "   0: (Feature(\"The model initiating step-by-step explanation\"), 1.433799342105263)\n",
       "   1: (Feature(\"Arithmetic operations in word problems\"), 1.1082763671875)\n",
       "   2: (Feature(\"Spherical play objects, especially bouncing balls\"), 1.0290305397727273)\n",
       "   3: (Feature(\"Types of car transmissions, especially automatic and semi-automatic\"), 0.8590666118421053)\n",
       "   4: (Feature(\"Small countable objects\"), 0.83154296875)\n",
       "   5: (Feature(\"Numerical values in math problems or quantitative descriptions\"), 0.8077713815789473)\n",
       "   6: (Feature(\"Basic Arithmetic and Simple Calculations\"), 0.7566856971153846)\n",
       "   7: (Feature(\"Math word problems involving financial calculations\"), 0.5727796052631579)\n",
       "   8: (Feature(\"Mathematical word problems involving counting animals or body parts\"), 0.4872036637931034)\n",
       "   9: (Feature(\"The model is formulating mathematical equations from word problems or scenarios\"), 0.4193522135416667)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_cot_context.top(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(\"<|begin_of_text|>\"),\n",
       " Token(\"<|start_header_id|>\"),\n",
       " Token(\"user\"),\n",
       " Token(\"<|end_header_id|>\"),\n",
       " Token(\"\n",
       " \n",
       " \"),\n",
       " Token(\"A\"),\n",
       " Token(\" store\"),\n",
       " Token(\" sells\"),\n",
       " Token(\" a\"),\n",
       " Token(\" product\"),\n",
       " Token(\" for\"),\n",
       " Token(\" $\"),\n",
       " Token(\"60\"),\n",
       " Token(\",\"),\n",
       " Token(\" and\"),\n",
       " Token(\" it\"),\n",
       " Token(\" costs\"),\n",
       " Token(\" $\"),\n",
       " Token(\"45\"),\n",
       " Token(\" to\"),\n",
       " Token(\" produce\"),\n",
       " Token(\".\"),\n",
       " Token(\" If\"),\n",
       " Token(\" the\"),\n",
       " Token(\" store\"),\n",
       " Token(\" sells\"),\n",
       " Token(\" \"),\n",
       " Token(\"500\"),\n",
       " Token(\" units\"),\n",
       " Token(\",\"),\n",
       " Token(\" what\"),\n",
       " Token(\" is\"),\n",
       " Token(\" the\"),\n",
       " Token(\" total\"),\n",
       " Token(\" profit\"),\n",
       " Token(\"?\"),\n",
       " Token(\" Lets\"),\n",
       " Token(\" think\"),\n",
       " Token(\" step\"),\n",
       " Token(\" by\"),\n",
       " Token(\" step\"),\n",
       " Token(\"<|eot_id|>\"),\n",
       " Token(\"<|start_header_id|>\"),\n",
       " Token(\"assistant\"),\n",
       " Token(\"<|end_header_id|>\"),\n",
       " Token(\"\n",
       " \n",
       " \"),\n",
       " Token(\"Let\"),\n",
       " Token(\"'s\"),\n",
       " Token(\" calculate\"),\n",
       " Token(\" the\"),\n",
       " Token(\" total\"),\n",
       " Token(\" profit\"),\n",
       " Token(\"!\n",
       " \"),\n",
       " Token(\"Cost\"),\n",
       " Token(\" to\"),\n",
       " Token(\" produce\"),\n",
       " Token(\" \"),\n",
       " Token(\"500\"),\n",
       " Token(\" units\"),\n",
       " Token(\":\"),\n",
       " Token(\" \"),\n",
       " Token(\"500\"),\n",
       " Token(\" x\"),\n",
       " Token(\" $\"),\n",
       " Token(\"45\"),\n",
       " Token(\" =\"),\n",
       " Token(\" $\"),\n",
       " Token(\"22\"),\n",
       " Token(\",\"),\n",
       " Token(\"500\"),\n",
       " Token(\"\n",
       " \"),\n",
       " Token(\"Revenue\"),\n",
       " Token(\":\"),\n",
       " Token(\" \"),\n",
       " Token(\"500\"),\n",
       " Token(\" x\"),\n",
       " Token(\" $\"),\n",
       " Token(\"60\"),\n",
       " Token(\" =\"),\n",
       " Token(\" $\"),\n",
       " Token(\"30\"),\n",
       " Token(\",\"),\n",
       " Token(\"000\"),\n",
       " Token(\"\n",
       " \"),\n",
       " Token(\"Total\"),\n",
       " Token(\" profit\"),\n",
       " Token(\":\"),\n",
       " Token(\" Revenue\"),\n",
       " Token(\" -\"),\n",
       " Token(\" Cost\"),\n",
       " Token(\" =\"),\n",
       " Token(\" $\"),\n",
       " Token(\"30\"),\n",
       " Token(\",\"),\n",
       " Token(\"000\"),\n",
       " Token(\" -\"),\n",
       " Token(\" $\"),\n",
       " Token(\"22\"),\n",
       " Token(\",\"),\n",
       " Token(\"500\"),\n",
       " Token(\" =\"),\n",
       " Token(\" $\"),\n",
       " Token(\"7\"),\n",
       " Token(\",\"),\n",
       " Token(\"500\"),\n",
       " Token(\"\n",
       " \"),\n",
       " Token(\"The\"),\n",
       " Token(\" store\"),\n",
       " Token(\"'s\"),\n",
       " Token(\" total\"),\n",
       " Token(\" profit\"),\n",
       " Token(\" would\"),\n",
       " Token(\" be\"),\n",
       " Token(\" $\"),\n",
       " Token(\"7\"),\n",
       " Token(\",\"),\n",
       " Token(\"500\"),\n",
       " Token(\"!\"),\n",
       " Token(\"<|eot_id|>\")]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_cot_context.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(\"500\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FeatureActivations(\n",
       "   0: (Feature(\"Final numerical answers in mathematical calculations\"), 2.1875)\n",
       "   1: (Feature(\"Large monetary amounts in thousands\"), 2.140625)\n",
       "   2: (Feature(\"The model is concluding its explanation or calculation\"), 1.4296875)\n",
       "   3: (Feature(\"Numerical values in text\"), 1.2890625)\n",
       "   4: (Feature(\"Step-by-step mathematical calculations and reasoning\"), 1.09375)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(with_cot_context.tokens[-3])\n",
    "\n",
    "token_acts = with_cot_context.tokens[-3].inspect()\n",
    "token_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Question: Q: Where do you put your grapes just before checking out? Answer Choices: (A) mouth (B) grocery cart (C) super market (D) fruit basket (E) fruit market\n",
      "A: The answer is (B).\n",
      "Q: Google Maps and other highway and street GPS services have replaced what? Answer Choices: (A) united states (B) mexico (C) countryside (D) atlas\n",
      "A: \n",
      "Direct Answer: I think I can help you out!\n",
      "\n",
      "The correct answer is (D) atlas!\n",
      "Cot Question: Q: Where do you put your grapes just before checking out? Answer Choices: (A) mouth (B) grocery cart (C) super market (D) fruit basket (E) fruit market\n",
      "A: The answer should be the place where grocery items are placed before checking out. Of the above choices, grocery cart makes the most sense for holding grocery items. So the answer is (B).\n",
      "Q: Google Maps and other highway and street GPS services have replaced what? Answer Choices: (A) united states (B) mexico (C) countryside (D) atlas\n",
      "A: \n",
      "Cot Answer: I think I can help you with that!\n",
      "\n",
      "Google Maps and other highway and street GPS services have replaced the atlas. So, the correct answer is (D) atlas.\n",
      "{'id': 20, 'steering_features': FeatureGroup([\n",
      "   0: \"The model is providing a final answer or conclusion\",\n",
      "   1: \"Introducing logical conclusions or inferences\",\n",
      "   2: \"Data structure and formatting elements\",\n",
      "   3: \"Emphasizing importance or certainty in statements\",\n",
      "   4: \"Detecting requests for structured creative scenarios or outlines\"\n",
      "])}\n"
     ]
    }
   ],
   "source": [
    "idx = 19\n",
    "question = top_features_list[idx]\n",
    "print(\"Direct Question:\",question[\"direct\"]+\"\\n\"+question[\"question\"])\n",
    "print(\"Direct Answer:\",question[\"direct_answer\"])\n",
    "\n",
    "print(\"Cot Question:\",question[\"cot\"]+\"\\n\"+question[\"question\"])\n",
    "\n",
    "print(\"Cot Answer:\",question[\"cot_answer\"])\n",
    "\n",
    "print(steering_features_list[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steering_features(question):\n",
    "    \n",
    "    _, cot_features = client.features.contrast(\n",
    "    dataset_1=[\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question[\"direct\"]+\"\\n\"+question[\"question\"]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": question[\"direct_answer\"]\n",
    "            }\n",
    "        ]\n",
    "    ],\n",
    "    dataset_2=[\n",
    "        \n",
    "            [ {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question[\"cot\"]+\"\\n\"+question[\"question\"]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": question[\"cot_answer\"]\n",
    "                }]\n",
    "            ],\n",
    "        dataset_2_feature_rerank_query=\"detailed explanation\",\n",
    "        model=variant,\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    return cot_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:31<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "pbar=tqdm(enumerate(questions),total=len(questions))\n",
    "steering_features_list = []\n",
    "for (k,question) in pbar:\n",
    "    \n",
    "    top_steering_features = get_steering_features(question)\n",
    "    steering_features_list.append({\"id\":k+1,\"steering_features\":top_steering_features})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steering Model according to steering features to check for CoT effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"few_shot_steering_feature_thresholds.json\",\"r\") as file:\n",
    "    steering_thresholds=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"math_qa_with_answers.csv\")\n",
    "test_dataset_sample = test_dataset.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The model is providing a list of options',\n",
       " 'Explanatory connectives in step-by-step reasoning',\n",
       " 'Framing mathematical or logical problem-solving steps',\n",
       " 'The model is providing scientific classification or technical explanation']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature(\"Polite and appreciative language in structured formal communication\") 0.5\n",
      "Variant(\n",
      "   base_model=meta-llama/Meta-Llama-3-8B-Instruct,\n",
      "   edits={\n",
      "      Feature(\"Polite and appreciative language in structured formal communication\"): {'mode': 'nudge', 'value': 0.5},\n",
      "   }\n",
      ")\n",
      "Feature(\"Data structure and formatting elements\") 0.3\n",
      "Variant(\n",
      "   base_model=meta-llama/Meta-Llama-3-8B-Instruct,\n",
      "   edits={\n",
      "      Feature(\"Data structure and formatting elements\"): {'mode': 'nudge', 'value': 0.3},\n",
      "   }\n",
      ")\n",
      "Feature(\"Introduction of explanatory statements in question-answering contexts\") 0.3\n",
      "Variant(\n",
      "   base_model=meta-llama/Meta-Llama-3-8B-Instruct,\n",
      "   edits={\n",
      "      Feature(\"Introduction of explanatory statements in question-answering contexts\"): {'mode': 'nudge', 'value': 0.3},\n",
      "   }\n",
      ")\n",
      "Feature(\"Step-by-step reasoning or problem-solving approach\") 0.4\n",
      "Variant(\n",
      "   base_model=meta-llama/Meta-Llama-3-8B-Instruct,\n",
      "   edits={\n",
      "      Feature(\"Step-by-step reasoning or problem-solving approach\"): {'mode': 'nudge', 'value': 0.4},\n",
      "   }\n",
      ")\n",
      "Feature(\"Action verbs in step-by-step instructions or processes\") 0.5\n",
      "Variant(\n",
      "   base_model=meta-llama/Meta-Llama-3-8B-Instruct,\n",
      "   edits={\n",
      "      Feature(\"Action verbs in step-by-step instructions or processes\"): {'mode': 'nudge', 'value': 0.5},\n",
      "   }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "variant.reset()\n",
    "base_variant = variant\n",
    "for key in steering_thresholds:\n",
    "    value = steering_thresholds[key]\n",
    "    searched_features, relevance = client.features.search(\n",
    "    key,\n",
    "    model=variant,\n",
    "    top_k=1)\n",
    "    steering_feature = searched_features[0]\n",
    "    \n",
    "    variant.reset()\n",
    "    print(steering_feature,value)\n",
    "    variant.set(steering_feature, value)\n",
    "    print(variant)\n",
    "    \n",
    "    test_dataset_sample[f'{key}_variant'] = test_dataset_sample['Problem'].apply(lambda x: query_model(variant,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_sample_part1 = pd.read_csv(\"test_data_results_part1.csv\")\n",
    "test_dataset_sample_part2 = pd.read_csv(\"test_data_results_part2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['The model is providing a list of options_variant',\n",
       "       'Explanatory connectives in step-by-step reasoning_variant',\n",
       "       'Framing mathematical or logical problem-solving steps_variant',\n",
       "       'The model is providing scientific classification or technical explanation_variant',\n",
       "       'base_variant'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_results = pd.concat([test_dataset_sample_part1,test_dataset_sample_part2[test_dataset_sample_part2.columns[2:]]],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Problem', 'answer',\n",
       "       'Polite and appreciative language in structured formal communication_variant',\n",
       "       'Data structure and formatting elements_variant',\n",
       "       'Introduction of explanatory statements in question-answering contexts_variant',\n",
       "       'Step-by-step reasoning or problem-solving approach_variant',\n",
       "       'Action verbs in step-by-step instructions or processes_variant'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant(\n",
      "   base_model=meta-llama/Meta-Llama-3-8B-Instruct,\n",
      "   edits={\n",
      "   }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "base_variant.reset()\n",
    "print(base_variant)\n",
    "test_dataset_sample['base_variant_dataset_1'] = test_dataset_sample['Problem'].apply(lambda x: query_model(base_variant,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_sample[['Problem','answer',\"base_variant_dataset_1\"]].to_csv(\"test_dataset1_resuls_BaseVariant.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A math problem! Let's break it down step by step.\\n\\nLet's say the two numbers are A and B, and their ratio is 3:4. This means that A is 3/4 of B.\\n\\nThe LCM (Least Common Multiple) of A and B is 180.\\n\\nSo, A = 3x and B = 4x, and their LCM is 180.\\n\\nTo find the values of A and B, we need to find the LCM of 3x and 4x, which is 180.\\n\\nThe LCM of 3x and 4x is 60x, because 3x and 4x are both multiples of 60.\\n\\nSo, A = 3x = 60 and B = 4x = 80.\\n\\nTherefore, the numbers are A = 60 and B = 80.\\n\\nDid that make sense?\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_model(variant,problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      2\u001b[0m     [\n\u001b[1;32m      3\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mHola\u001b[39;49m\u001b[39m\"\u001b[39;49m}\n\u001b[1;32m      4\u001b[0m     ],\n\u001b[1;32m      5\u001b[0m     model\u001b[39m=\u001b[39;49mvariant,\n\u001b[1;32m      6\u001b[0m     stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      7\u001b[0m     max_completion_tokens\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m\n\u001b[1;32m      8\u001b[0m ):\n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(token\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdelta\u001b[39m.\u001b[39mcontent, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/apart_hackathon/lib/python3.9/site-packages/goodfire/api/chat/client.py:110\u001b[0m, in \u001b[0;36mChatAPICompletions.create\u001b[0;34m(self, messages, model, stream, max_completion_tokens, top_p, temperature, stop, timeout, seed, _ChatAPICompletions__system_prompt)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mbase_model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Meta-Llama-3.1-70B-Instruct\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    108\u001b[0m         scale \u001b[39m=\u001b[39m \u001b[39m1.3\u001b[39m\n\u001b[0;32m--> 110\u001b[0m     payload[\u001b[39m\"\u001b[39m\u001b[39mcontroller\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mcontroller\u001b[39m.\u001b[39;49mjson(scale\u001b[39m=\u001b[39;49mscale)\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    114\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m_stream_response\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Generator[StreamingChatCompletionChunk, Any, Any]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/apart_hackathon/lib/python3.9/site-packages/goodfire/controller/controller.py:162\u001b[0m, in \u001b[0;36mController.json\u001b[0;34m(self, scale)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjson\u001b[39m(\u001b[39mself\u001b[39m, scale: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m--> 162\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minterventions\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    163\u001b[0m             intervention\u001b[39m.\u001b[39mjson(scale\u001b[39m=\u001b[39mscale) \u001b[39mfor\u001b[39;00m intervention \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interventions\n\u001b[1;32m    164\u001b[0m         ],\n\u001b[1;32m    165\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mscopes\u001b[39m\u001b[39m\"\u001b[39m: [scope\u001b[39m.\u001b[39mjson(scale\u001b[39m=\u001b[39mscale) \u001b[39mfor\u001b[39;00m scope \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scopes],\n\u001b[1;32m    166\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    167\u001b[0m     }\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/apart_hackathon/lib/python3.9/site-packages/goodfire/controller/controller.py:163\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjson\u001b[39m(\u001b[39mself\u001b[39m, scale: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    162\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minterventions\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m--> 163\u001b[0m             intervention\u001b[39m.\u001b[39;49mjson(scale\u001b[39m=\u001b[39;49mscale) \u001b[39mfor\u001b[39;00m intervention \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interventions\n\u001b[1;32m    164\u001b[0m         ],\n\u001b[1;32m    165\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mscopes\u001b[39m\u001b[39m\"\u001b[39m: [scope\u001b[39m.\u001b[39mjson(scale\u001b[39m=\u001b[39mscale) \u001b[39mfor\u001b[39;00m scope \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scopes],\n\u001b[1;32m    166\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    167\u001b[0m     }\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/apart_hackathon/lib/python3.9/site-packages/goodfire/controller/controller.py:19\u001b[0m, in \u001b[0;36mIntervention.json\u001b[0;34m(self, scale)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjson\u001b[39m(\u001b[39mself\u001b[39m, scale: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     18\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode,\n\u001b[0;32m---> 19\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures\u001b[39m.\u001b[39;49mjson(),\n\u001b[1;32m     20\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: (\n\u001b[1;32m     21\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mjson()\n\u001b[1;32m     22\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue, \u001b[39mfloat\u001b[39m)\n\u001b[1;32m     23\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue \u001b[39m*\u001b[39m scale\n\u001b[1;32m     24\u001b[0m         ),\n\u001b[1;32m     25\u001b[0m     }\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/apart_hackathon/lib/python3.9/site-packages/goodfire/features/features.py:256\u001b[0m, in \u001b[0;36mFeatureGroup.json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjson\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m: [f\u001b[39m.\u001b[39mjson() \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_features\u001b[39m.\u001b[39mvalues()]}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/apart_hackathon/lib/python3.9/site-packages/goodfire/features/features.py:256\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjson\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m: [f\u001b[39m.\u001b[39;49mjson() \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_features\u001b[39m.\u001b[39mvalues()]}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'json'"
     ]
    }
   ],
   "source": [
    "for token in client.chat.completions.create(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Hola\"}\n",
    "    ],\n",
    "    model=variant,\n",
    "    stream=True,\n",
    "    max_completion_tokens=1000\n",
    "):\n",
    "    print(token.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apart_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86e3c79ddc639aa5b7b327e0fd3b4d322aa8dd7a6e00a495630889d803b7d179"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
